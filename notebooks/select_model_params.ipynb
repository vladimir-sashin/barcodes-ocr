{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c1d827-e428-4c2e-ba37-08dd83e0cd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "from os import path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from PIL import Image\n",
    "from timm import create_model\n",
    "\n",
    "sys.path.append(path.abspath('..'))\n",
    "\n",
    "from src.train.config.train_config import get_train_cfg  # noqa: E402\n",
    "from src.train.data_utils.dataset_finder import find_dataset  # noqa: E402\n",
    "from src.train.data_utils.transforms import PadResizeOCR  # noqa: E402\n",
    "from src.train.dataset import BarcodeDataset  # noqa: E402\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "warnings.simplefilter('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b541b54-6215-4568-ae0b-0422c4836dad",
   "metadata": {},
   "source": [
    "# Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b65194d-36bb-4f96-85e1-6d0b5da8237d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_train_cfg()\n",
    "dataset_splits = find_dataset(cfg.datamodule_cfg.data_source_cfg)\n",
    "train_paths = dataset_splits.train\n",
    "train_dataset = BarcodeDataset(anns_path=train_paths.ann_file, data_folder=train_paths.img_folder)\n",
    "df = pd.read_csv(train_paths.ann_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232e4190-51bb-40c6-b244-06de713d85d2",
   "metadata": {},
   "source": [
    "# Find the height for resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7929eee-27a6-422c-bab2-8ca9ce9ee3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_shapes = []\n",
    "for i in range(len(train_dataset)):\n",
    "    train_shapes.append(train_dataset[i][0].shape)\n",
    "train_shapes_np = np.array(train_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1cdb73-2163-4607-8e9c-f6070e2a2f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(train_shapes_np[:, 0])\n",
    "_ = sns.distplot(train_shapes_np[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb04e249-ee03-402a-897d-94770bb2693f",
   "metadata": {},
   "source": [
    "## Original images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e27cad-5033-475b-bdca-fb21c13a5394",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    cv_image = train_dataset[i][0]\n",
    "    Image.fromarray(cv_image)\n",
    "    cv_image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db8eb68-c336-499f-adf1-6bd34865dbe0",
   "metadata": {},
   "source": [
    "## Rescale images by height\n",
    "\n",
    "Images can be rescaled down without significant loss of information. Let's pick a height divisible by 32 and look at the barcodes: 96."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7737485-b376-4711-968e-f7b2283bb1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_height = 96\n",
    "\n",
    "for i in range(10):\n",
    "    image = train_dataset[i][0]\n",
    "    scale = new_height / image.shape[0]\n",
    "    scaled_image = cv2.resize(image, None, fx=scale, fy=scale)\n",
    "    Image.fromarray(scaled_image)\n",
    "    scaled_image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f188935-3288-47c5-9efe-a8aee51c0102",
   "metadata": {},
   "source": [
    "In fact, barcodes are still readable after resize of height to 96."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58fdeec-906e-488b-a333-bf8bcbdfe61d",
   "metadata": {},
   "source": [
    "## Find width for letterbox resizing\n",
    "\n",
    "We will resize images keeping their aspect ratios and using zero padding (letterbox resize), so we need to find a new width. Let's take a look at the distribution of images width after resizing to the new height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e659d97c-c0e0-4e0b-973e-d7a224a4dad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_width = train_shapes_np[:, 1] * new_height / train_shapes_np[:, 0]\n",
    "np.max(train_width)\n",
    "_ = sns.distplot(train_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86eb895-2be6-49f5-a124-dac241aa5b3c",
   "metadata": {},
   "source": [
    "Here we see a few outliers, so let's filter them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c185ce70-ad6c-4d63-b853-63fced96eb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = np.percentile(train_width, 25)\n",
    "q3 = np.percentile(train_width, 75)\n",
    "iqr = q3 - q1\n",
    "lower_bound = q1 - 1.5 * iqr\n",
    "upper_bound = q3 + 1.5 * iqr\n",
    "\n",
    "train_width_filtered = train_width[(train_width > lower_bound) & (train_width < upper_bound)]\n",
    "np.max(train_width_filtered)\n",
    "sns.distplot(train_width_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd18a8b0-19f5-42b4-ab9c-4a6d7da46b09",
   "metadata": {},
   "source": [
    "To get the new widths of the images we'll take the max value and round it up to the next value divisible by 32. Given the max width of 322, the new width divisible by 32 is 352. Let's apply this resize to barcodes and see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7451a32-f86a-4732-95e0-ac587c98a511",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_width = 352\n",
    "transform = PadResizeOCR(target_width=new_width, target_height=new_height, mode='left')\n",
    "\n",
    "for i in range(10):\n",
    "    image = train_dataset[i][0]\n",
    "    transformed_image = transform(image=image)['image']\n",
    "    Image.fromarray(transformed_image)\n",
    "    transformed_image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b437b6-babd-47ea-8e21-cb9a02ff9201",
   "metadata": {},
   "source": [
    "# Find backbone feature map size\n",
    "\n",
    "To get the best of CNN + RNN + CTC architecture, we need to find an optimal size of the feature map returned by CNN backbone. Feature map is sliced before inputting to LSTM, and the recommended number of slices per character is used to 3 or more.\n",
    "\n",
    "Since maximum number of characters is 13 and max width is 323, we have: 323 / (13 * 3) = 8.28, rounded up to 9 pixels per slice. But as we resized width to 352, the featuremap must have at least 352 / 9 = 39.11 -> 40 slices.\n",
    "\n",
    "Let's pick a small and fast backbone like `mobilenetv3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55e099e-fbff-4a00-85b2-afea3f927867",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = create_model(\n",
    "    'resnet18',\n",
    "    pretrained=True,\n",
    "    features_only=True,\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    feature_maps = backbone(torch.rand(1, 3, 96, 352))\n",
    "\n",
    "for layer_idx, feature_map in enumerate(feature_maps):\n",
    "    print(layer_idx + 1, feature_map.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cde6270-336e-4269-b1a9-278ed75b56ea",
   "metadata": {},
   "source": [
    "Feature maps from layers #4 and #5 are too narrow, maps from layers #1 and #2 are too wide.\n",
    "\n",
    "Finally, feature map from layer #3 seems to fit, since its widths is 44 > required 40.\n",
    "\n",
    "This should work for a baseline, however, there is a room for improvement by tuning backbone and RNN parameters. For example, we can try a different backbone and set `output_stride` parameter to get a feature map of suitable size from a deeper layer like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3567351-d80b-40fd-8ede-6a8756526139",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone_stride = create_model(\n",
    "    'resnet34',\n",
    "    pretrained=True,\n",
    "    features_only=True,\n",
    "    output_stride=8,\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    feature_maps_stride = backbone_stride(torch.rand(1, 3, 96, 352))\n",
    "\n",
    "for layer_idx, feature_map_stride in enumerate(feature_maps_stride):\n",
    "    print(layer_idx + 1, feature_map_stride.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9450eb99-d775-4d88-9953-feec6d662779",
   "metadata": {},
   "source": [
    "This way we can take feature map from 4th or even 5th level since these maps will have a suitable width of 44 > 40.\n",
    "\n",
    "**Important notes regarding RNN hyperparameters.**\n",
    "\n",
    "Let `(BS x C_fm x H_fm x W_fm)` be the shape of feature map. In our RCNN, it's processed as follows:\n",
    "1. 1x1 conv (gate) is applied to the feature map to change the number of channels to `rnn_features_num`. Output: `(BS x rnn_features_num x H_fm x W_fm)`\n",
    "2. `W_fm` vertical slices are created out of this feature map, where length of each slice is `H_fm * rnn_features_num = rnn_input_size`. Output: `(W_fm x BS x rnn_input_size)`.\n",
    "3. This tensor is passed to RNN with `hidden_size`, outputting  `(W_fm x BS x hidden_size)` (`hidden_size * 2` in case of bidirectional RNN.\n",
    "4. RNN output is passed to a linear layer with softmax.\n",
    "\n",
    "Thus:\n",
    "1. `rnn_features_num` is the new number of channels of backbone's output feature map.\n",
    "2. `rnn_input_size` is the length of vertical slices fed to RNN across the entire width of the feature map. Must be set equal to `H_fm * rnn_features_num`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501671b6-a9e2-4759-86b6-b7459149ede5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
